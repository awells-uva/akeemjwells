---
title: "Final Part 1"
author: "Akeem Wells"
date: "10/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings=FALSE, message = FALSE,fig.align = "center")
```

```{r}
# ggpairs
library(ggplot2)
library(GGally)
library(hrbrthemes)
# %>%
library(tidyverse)

# kfold
library(caret)
kfoldcv = 10

# lda
library(MASS)

# prediction
library(ROCR)

#partimat
library(klaR)
library(gridExtra)
library(yardstick)
library(plotly)
library(kableExtra)
```

# Introduction

In this project, you will use classification methods covered in this course to solve a real historical data mining problem: locating displaced persons living in makeshift shelters following the destruction of the earthquake in Haiti in 2010.

Following that earthquake, rescue workers, mostly from the United States military, needed to get food and water to the displaced persons. But with destroyed communications, impassable roads, and thousands of square miles, actually locating the people who needed help was challenging. As part of the rescue effort, a team from the Rochester Institute of Technology were flying an aircraft to collect high resolution geo-referenced imagery. 

```{r}
HaitiPixels <- read.csv(file = '/Users/awells/UVA/SYS6018/Disaster_Relief_Project/HaitiPixels.csv')
head(HaitiPixels)

HaitiPixels$Class <- as.factor(HaitiPixels$Class)
#summary(HaitiPixels)

#### Delete these 2 lines when ready for full set
#data <- sample_n(HaitiPixels, 10000)
#HaitiPixels <- data
#### 

dim(HaitiPixels)
summary(HaitiPixels)
attach(HaitiPixels)
```

It was known that the people whose homes had been destroyed by the earthquake were creating temporary shelters using blue tarps, and these blue tarps would be good indicators of where the displaced persons were – if only they could be located in time, out of the thousands of images that would be collected every day. 

```{r}
detach(HaitiPixels)
# Blue Tarp
BlueTarp_rest <- rep("Down",length(HaitiPixels$Class))
BlueTarp_rest[HaitiPixels$Class == "Blue Tarp"] <- "Up"

HaitiPixels <- data.frame(HaitiPixels,BlueTarp_rest )

HaitiPixels$BlueTarp_rest <- as.factor(HaitiPixels$BlueTarp_rest)

summary(HaitiPixels)
attach(HaitiPixels)
BlueTarp_rest <- as.factor(BlueTarp_rest)
```

The problem was that there was no way for aid workers to search the thousands of images in time to find the tarps and communicate the locations back to the rescue workers on the ground in time. The solution would be provided by data-mining algorithms, which could search the images far faster and more thoroughly (and accurately?) then humanly possible.

The goal was to find an algorithm that could effectively search the images in order to locate displaced persons and communicate those locations rescue workers so they could help those who needed it in time. This disaster relief project is the subject matter for your project in this course, which you will submit in two parts. You will use data from the actual data collection process was carried out over Haiti. Your goal is to test each of the algorithms you learn in this course on the imagery data collected during the relief efforts made Haiti in response to the 2010 earthquake, and determine which method you will use to as accurately as possible, and in as timely a manner as possible, locate as many of the displaced persons identified in the imagery data so that they can be provided food and water before their situations become unsurvivable.

```{r}
HaitiPixels %>% ggpairs(upper=list(continuous=wrap("cor", size=3)))
```

## Setup

For the set up of the data set ( HaitiPixels, $n=$ `r toString(nrow(HaitiPixels))`) a likely decision is to use a one vs rest approach.  Our data is a multi-class classification problem with binary classifications: 

* Blue Tarp  vs [  Rooftop , Soil , Various Non-Tarp, Vegetation ]

* Rooftop vs [ Blue Tarp,  Soil , Various Non-Tarp, Vegetation ]

* Soil vs [ Blue Tarp,  Rooftop , Various Non-Tarp, Vegetation ]

* Various Non-Tarp vs [ Blue Tarp,  Rooftop , Soil , Vegetation ]

* Vegetation vs [ Blue Tarp,  Rooftop , Soil , Various Non-Tarp ]

```{r}
# Get Final Hold - Out 5% of Data ~3000 observations
data.fho = sort(sample(nrow(HaitiPixels), nrow(HaitiPixels)*.05))

fho <- HaitiPixels[data.fho,]
data_remain <- HaitiPixels[-data.fho,]

#80-20 split Train-Test

data_minus_fho = sort(sample(nrow(data_remain), nrow(data_remain)*.8))
train <-  data_remain[data_minus_fho,]
test <- data_remain[-data_minus_fho,]

# shuffle just to be safe
train <- dplyr::sample_n(train, nrow(train))
test <- dplyr::sample_n(test, nrow(test))
fho <- dplyr::sample_n(fho, nrow(fho))

test.BlueTarp <- as.factor(test$BlueTarp_rest)
train$BlueTarp_rest <- as.factor(train$BlueTarp_rest)
levels(train$BlueTarp_rest) <- c("Down","Up")

#paste("Length of Train: ", nrow(train))
#paste("Length of Test: ", nrow(test))
#paste("Length of Final Hold Out: ", nrow(fho))

```

Since we are looking for  temporary shelters made using blue tarps, we generated a variable named “BlueTarp_rest”, a binary classifier that has two levels 

* “Up” - if a blue tarp was visible (or up )

* “Down” - if a blue tarp was not visible.

We'll take a random sample of the HaitiPixels dataset that’s 5% of the data set. We’ll name this the FHO, or Final HoldOut Dataset to be used in Part 2 ( Hold-Out Test Sample Performance). We randomly take the remaining data and divide it into an 80% training set and  20% test set. This leaves a training set of $n=$ `r toString(nrow(train))`, a test set of $n=$ `r toString(nrow(test))` and a FHO of $n=$ `r toString(nrow(fho))`

# Analysis

# KNN

The first classification process that we used is the K-nearest neighbors regression. Given a value for K and a prediction point $x_0$, KNN regression first identifies the K training observations that are closest to $x_0$, represented by $N_0$.  The optimal value for K will depend on the bias-variance tradeoff. A small value for K provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation. In contrast, larger values of K provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. We’ll use the caret package in R to apply the KNN classification procedure using a 10 - Fold Cross-Validation evaluation procedure. 

```{r caret KNN}
set.seed(1)

# https://blog.revolutionanalytics.com/2016/05/using-caret-to-compare-models.html
classifier <- caret::train(BlueTarp_rest ~ Red + Green + Blue, data=train, method="knn", 
              preProcess=c("center","scale"), 
              tuneGrid=data.frame(k=seq(1,75,1)),
              #tuneGrid=data.frame(k=seq(1,200,2)),
              #tuneGrid=data.frame(k=seq(1,500,5)),
              trControl = caret::trainControl("cv", number=kfoldcv, 
                          returnResamp='all', savePredictions='final',
                          classProbs=TRUE))
```

```{r}
classifier
bestk = classifier$bestTune
```

Testing across various values of K, we can see that our optimal K is `r toString(bestk)`.  So we’ll proceed with the analysis using this value. 

```{r}
classifier %>% ggplot(aes(x=seq_along(Accuracy), y=Accuracy))
```

```{r message=FALSE, warning=FALSE}
classifier$resample %>% 
  dplyr::group_split(Resample) %>% 
  purrr::map(rowid_to_column) %>%
  dplyr::bind_rows() %>%
  ggplot(aes(rowid, Accuracy)) + geom_point() +
  geom_smooth(formula='y~x', method='loess', span=.03) +
  geom_line(classifier$results, mapping=aes(seq_along(Accuracy), Accuracy),
            size=2, color='red')
```

Next we'll look at the Confusion Matrix from the model. We'll use the image below as a reference to analyzing the matrix.

![Confusion Matrix Reference](/Users/awells/UVA/SYS6018/Disaster_Relief_Project/confmat.png)

```{r warning=FALSE}
# confusionMatrix
THRESHOLD <- 0.5
out_of_folds_CM <- classifier$pred %>% 
  dplyr::mutate(pred2 = ifelse(Up>THRESHOLD, 'Up', 'Down')) %>%
  dplyr::mutate(pred2 = factor(pred2, levels=c('Down','Up'))) %>%
  dplyr::group_split(Resample) %>%
  purrr::map(~ caret::confusionMatrix(data=.x$pred2, reference=.x$obs))

out_of_folds_CM_vis <- out_of_folds_CM %>% 
  purrr::map(~ .x$table %>% broom::tidy() %>%
                 ggplot(aes(x=Reference, y=Prediction, fill=n, label=n)) +
                 geom_tile() + geom_text(aes(label=n), size=5, color='white'))
 
gridExtra::grid.arrange(grobs=out_of_folds_CM_vis, ncol=2)
```

From the 10-Fold cross validation we can see we have our true positive references to a blue tarp. We can measure the sensitivity also known as the true positive rate. This is the fraction of observations that are correctly identified, using a given threshold value. The initial default threshold is `r toString(THRESHOLD)`. The false positive rate is 1-specificity; the fraction of observations that we classify incorrectly, using that same threshold value of `r toString(THRESHOLD)`. We can view the receiver operating characteristic curve or ROC curve to measure these two statistics. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate.

```{r}
options(yardstick.event_first = FALSE)
ROC_curve <- predict(classifier, type='prob') %>% 
  yardstick::roc_curve(truth=train$BlueTarp_rest, estimate=Up) %>%
  dplyr::mutate(one_minus_specificity = 1-specificity)

# Sensitivity: TP/(TP+FN)
# 1-Specificity: FP/(TN+FP)

ROC_curve_plot <- ROC_curve %>%
  ggplot(aes(x=one_minus_specificity, y=sensitivity)) + 
  geom_line() + geom_point() +
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC curve')

ggplotly(ROC_curve_plot)

predict(classifier, type='prob') %>% 
  yardstick::roc_auc(truth=train$BlueTarp_rest, Up) 
```


```{r}
ROC_curve %>% head()
predict(classifier, type='prob') %>% head()
```

We can also look at each ROC curve from each fold from the KNN model.

```{r}
ROC_curveS <- classifier$pred %>%
  dplyr::group_split(Resample) %>% 
  purrr::map(~ yardstick::roc_curve(.x, truth=obs, Up)) %>%
  purrr::map(~ dplyr::mutate(.x, one_minus_specificity = 1-specificity)) %>%
  purrr::map(~ geom_line(.x, mapping=aes(one_minus_specificity, sensitivity)))

ROC_curveS[[1]] <- ggplot() + ROC_curveS[[1]]
Reduce("+", ROC_curveS) + 
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC Curves')
```

To find the optimal threshold, well look at using the $F_1$ Score.

$F_1$ score is more useful measure than accuracy for problems with uneven class distributions since $F_1$ is defined as :

\[ F_1 = 2 * \frac{\frac{\Sigma True Positive}{\Sigma Predicted Condition Positive}*\frac{\Sigma True Positive}{\Sigma Condition Positive}}{\frac{\Sigma True Positive}{\Sigma Predicted Condition Positive}+\frac{\Sigma True Positive}{\Sigma Condition Positive}}\]

it takes into account both false positive and false negatives. The optimal $F_1$ Score is between 0 and 1.

```{r}
tuned_call <- function(preds, decision_rule_threshold){
  calls <- rep('Down', length(preds))
  calls[preds>decision_rule_threshold]='Up'
  factor(calls, levels=c('Down','Up'))
}


thresholds = seq(0.01,0.99,.01) # Avoid Threshold Tails
F_1_scores <- rep(NA, length(thresholds))
GMeans <- rep(NA, length(thresholds))

preds <- predict(classifier, type='prob')[,2]
y <- train$BlueTarp_rest
for (i in 1:length(thresholds)){
  yhat <- tuned_call(preds, thresholds[i])
  one_minus_specificity <- mean(yhat[y=='Down']=='Up')
  sensitivity <- mean(yhat[y=='Up']=='Up')
  one_minus_specificity
  sensitivity
  results <- tibble::tibble(y=y, yhat=yhat)
  
  caret::confusionMatrix(yhat, y, positive = "Up") 
  cm <- results %>%yardstick::conf_mat(truth=y, yhat)
  
  trueNegative  = cm$table[1,1]
  falseNegative = cm$table[1,2]
  falsePositive = cm$table[2,1]
  truePositve   = cm$table[2,2]
  
  #paste0("True Positive: ", truePositve )
  #paste0("False Negative: ", falseNegative )
  #paste0("True Negative: ", trueNegative )
  #paste0("False Positive: ", falsePositive )
  conditionPositive = truePositve + falseNegative
  conditionNegative = trueNegative + falsePositive 
  predictedConditionPositive = truePositve +  falsePositive 
  predictedConditionNegative = trueNegative + falseNegative
  
  TotalPopulation = truePositve + falsePositive + trueNegative + falseNegative
  
  prevalence = conditionPositive / TotalPopulation
  accuracy = (truePositve + trueNegative) / TotalPopulation
  PPV = truePositve / predictedConditionPositive
  FDR = falsePositive / predictedConditionPositive
  FOR = falseNegative / predictedConditionNegative
  NPV = trueNegative / predictedConditionNegative
  TPR = truePositve / conditionPositive
  FPR = falsePositive / conditionNegative
  FNR = falseNegative / conditionPositive
  SPC = TNR = trueNegative / conditionNegative
  LR.PLUS = TPR / FPR
  LR.MINUS = FNR/TNR
  DOR = LR.PLUS / LR.MINUS
  
  F_1_score = 2 * ((PPV * TPR)/(PPV + TPR))
  F_1_scores[i] <- F_1_score
  GMeans[i] = sqrt(TPR*SPC)
}

opt_thresh = thresholds[tail(which(na.omit(F_1_scores) == min(na.omit(F_1_scores))), n=1)]

if (opt_thresh < 0.1){opt_thresh = 0.1}
if (opt_thresh > 0.5){opt_thresh = 0.5}

#opt_thresh

#points(opt_thresh, F_1_scores[tail(which(na.omit(F_1_scores) == min(na.omit(F_1_scores))), n=1)], col = 'red', cex = 2, pch = 19)
```

If the optimal threshold is less than 0.1, we will set our optimal threshold to 0.1. We do this because we do not want to be too specific in our threshold. In real world example, we have to live with the fact that we cannot help all those affected due to limited resources. Our goal is to help as many as those affected, so we want to have a little more flexibility in our threshold.

```{r}
plot(thresholds, F_1_scores, xlab = 'Thresholds', ylab = 'F1 Score', type = 'l')
```

```{r}
plot(thresholds, GMeans, xlab = 'Thresholds', ylab = 'Geometric Mean', type = 'l')
thresholds[tail(which(na.omit(GMeans) == max(na.omit(GMeans))), n=1)]
```


```{r}
paste0("Confusion Matrix 0.5")
y <- train$BlueTarp_rest
yhat <- tuned_call(preds, 0.5)
one_minus_specificity <- mean(yhat[y=='Down']=='Up')
sensitivity <- mean(yhat[y=='Up']=='Up')
results <- tibble::tibble(y=y, yhat=yhat)

caret::confusionMatrix(yhat, y, positive = "Up") 
cm <- results %>%yardstick::conf_mat(truth=y, yhat)
```

From the initial threshold we can see:

* True Negative = `r toString(trueNegative )`

* False Negative = `r toString(falseNegative)`

* False Positive = `r toString(falsePositive)`

* True Positive = `r toString(truePositve)`

* Condition Positive = `r toString(conditionPositive)`

* condition Negative = `r toString(conditionNegative)`

* Predicted Condition Positive = `r toString(predictedConditionPositive)`

* Predicted Condition Negative = `r toString(predictedConditionNegative)`

* Total Population = `r toString(TotalPopulation)`

* Prevalence = `r toString(prevalence)`

* Accuracy = `r toString(accuracy)`

* Positive predictive value, precision = `r toString(PPV)`

* False Discovery rate = `r toString(FDR)`

* False Omission Rate = `r toString(FOR)`

* Negative Predictive Value = `r toString(NPV)`

* True Positive Rate, Recall, Sensitivity = `r toString(TPR)`

* False Positive Rate = `r toString(FPR)`

* False Negative Rate = `r toString(FNR)`

* Specificity, Selectivity, True Negative rate = `r toString(SPC)`

* Positive likelihood ratio = `r toString(LR.PLUS)`

* Negative likelihood ratio = `r toString(LR.MINUS)`

* Diagnostic Odds Ratio = `r toString(DOR)`

* $F_1$ Score  = `r toString(F_1_score)`

From our plot we selected our optimal threshold as `r toString(opt_thresh)`. 

```{r}
yhat <- tuned_call(preds, opt_thresh)
one_minus_specificity <- mean(yhat[y=='Down']=='Up')
sensitivity <- mean(yhat[y=='Up']=='Up')
one_minus_specificity
sensitivity
results <- tibble::tibble(y=y, yhat=yhat)

caret::confusionMatrix(yhat, y, positive = "Up") 
cm <- results %>%yardstick::conf_mat(truth=y, yhat)

trueNegative  = cm$table[1,1]
falseNegative = cm$table[1,2]
falsePositive = cm$table[2,1]
truePositve   = cm$table[2,2]

conditionPositive = truePositve + falseNegative
conditionNegative = trueNegative + falsePositive 
predictedConditionPositive = truePositve +  falsePositive 
predictedConditionNegative = trueNegative + falseNegative

TotalPopulation = truePositve + falsePositive + trueNegative + falseNegative

prevalence = conditionPositive / TotalPopulation
accuracy = (truePositve + trueNegative) / TotalPopulation
PPV = truePositve / predictedConditionPositive
FDR = falsePositive / predictedConditionPositive
FOR = falseNegative / predictedConditionNegative
NPV = trueNegative / predictedConditionNegative
TPR = truePositve / conditionPositive
FPR = falsePositive / conditionNegative
FNR = falseNegative / conditionPositive
SPC = TNR = trueNegative / conditionNegative
LR.PLUS = TPR / FPR
LR.MINUS = FNR/TNR
DOR = LR.PLUS / LR.MINUS

F_1_score = 2 * ((PPV * TPR)/(PPV + TPR))
```

Now using the optimal threshold we get:

* True Negative = `r toString(trueNegative )`

* False Negative = `r toString(falseNegative)`

* False Positive = `r toString(falsePositive)`

* True Positive = `r toString(truePositve)`

* Condition Positive = `r toString(conditionPositive)`

* condition Negative = `r toString(conditionNegative)`

* Predicted Condition Positive = `r toString(predictedConditionPositive)`

* Predicted Condition Negative = `r toString(predictedConditionNegative)`

* Total Population = `r toString(TotalPopulation)`

* Prevalence = `r toString(prevalence)`

* Accuracy = `r toString(accuracy)`

* Positive predictive value, precision = `r toString(PPV)`

* False Discovery rate = `r toString(FDR)`

* False Omission Rate = `r toString(FOR)`

* Negative Predictive Value = `r toString(NPV)`

* True Positive Rate, Recall, Sensitivity = `r toString(TPR)`

* False Positive Rate = `r toString(FPR)`

* False Negative Rate = `r toString(FNR)`

* Specificity, Selectivity, True Negative rate = `r toString(SPC)`

* Positive likelihood ratio = `r toString(LR.PLUS)`

* Negative likelihood ratio = `r toString(LR.MINUS)`

* Diagnostic Odds Ratio = `r toString(DOR)`

* $F_1$ Score  = `r toString(F_1_score)`

```{r warning=FALSE}
# confusionMatrix

THRESHOLD <- opt_thresh
out_of_folds_CM <- classifier$pred %>% 
  dplyr::mutate(pred2 = ifelse(Up>THRESHOLD, 'Up', 'Down')) %>%
  dplyr::mutate(pred2 = factor(pred2, levels=c('Down','Up'))) %>%
  dplyr::group_split(Resample) %>%
  purrr::map(~ caret::confusionMatrix(data=.x$pred2, reference=.x$obs))

out_of_folds_CM_vis <- out_of_folds_CM %>% 
  purrr::map(~ .x$table %>% broom::tidy() %>%
                 ggplot(aes(x=Reference, y=Prediction, fill=n, label=n)) +
                 geom_tile() + geom_text(aes(label=n), size=5, color='white'))
 
gridExtra::grid.arrange(grobs=out_of_folds_CM_vis, ncol=2)
```

## Summary Stats K Nearest Neighbor

```{r}
options(yardstick.event_first = FALSE)
ROC_curve <- predict(classifier, type='prob') %>% 
  yardstick::roc_curve(truth=train$BlueTarp_rest, estimate=Up) %>%
  dplyr::mutate(one_minus_specificity = 1-specificity)

# Sensitivity: TP/(TP+FN)
# 1-Specificity: FP/(TN+FP)

ROC_curve_plot <- ROC_curve %>%
  ggplot(aes(x=one_minus_specificity, y=sensitivity)) + 
  geom_line() + geom_point() +
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC curve')

ggplotly(ROC_curve_plot)

temp = predict(classifier, type='prob') %>% yardstick::roc_auc(truth=train$BlueTarp_rest, Up) 

knn.Accuracy = accuracy
knn.AUC = temp$.estimate

twoclassSum = twoClassSummary( data = data.frame(obs = classifier$pred$obs,
  pred = classifier$pred$pred,
  Down = classifier$pred$Down, 
  Up = classifier$pred$Up), lev = levels(classifier$pred$obs))

knn.ROC = "TRUE" #unname(twoclassSum[1])
knn.Threshold = opt_thresh
knn.Sensitivity = knn.Recall= knn.Power =  TPR
knn.Specificity= 1-FPR
knn.FDR = FDR
knn.Precision=PPV
```

```{r}

df = data.frame(c(knn.Accuracy, knn.AUC, knn.ROC, knn.Threshold, knn.Sensitivity, knn.Specificity, knn.FDR , knn.Precision))


colnames(df) <- c(paste0("KNN (k =",bestk,")"))
rownames(df) <- c("Accuracy","AUC", "ROC", "Threshold","Sensitivity=Recall=Power","Specificity=1-FPR","FDR","Precision=PPV")
df %>% kbl() %>% kable_styling()
```

# LDA 

The next classification process that we used is the linear discriminant analysis (LDA). LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class specific mean vector and a covariance matrix that is common to all K classes. Again, we’ll use the caret package in R to apply the LDA classification procedure using a 10 - Fold Cross-Validation evaluation procedure. 

```{r caret LDA}
set.seed(1)
train$BlueTarp_rest <- as.factor(train$BlueTarp_rest)
levels(train$BlueTarp_rest) <- c("Down","Up")
# https://blog.revolutionanalytics.com/2016/05/using-caret-to-compare-models.html

classifier <- caret::train(BlueTarp_rest ~ Red + Green + Blue, data=train, method="lda", 
              preProcess=c("center","scale"),
              #tuneGrid=data.frame(.dimen = seq(1,10,1)),#seq(1,1000,5)),
              trControl = caret::trainControl("cv", number=kfoldcv, 
              returnResamp='all', savePredictions='final',
              classProbs=TRUE))

```

```{r}
classifier
```

```{r message=FALSE, warning=FALSE}
classifier$resample %>% 
  dplyr::group_split(Resample) %>% 
  purrr::map(rowid_to_column) %>%
  dplyr::bind_rows() %>%
  ggplot(aes(rowid, Accuracy)) + geom_point() +
  geom_smooth(formula='y~x', method='loess', span=.03) +
  geom_line(classifier$results, mapping=aes(seq_along(Accuracy), Accuracy),
            size=2, color='red')
```

Next we'll look at the Confusion Matrix from the model. 

```{r warning=FALSE}
# confusionMatrix
THRESHOLD <- 0.5
out_of_folds_CM <- classifier$pred %>% 
  dplyr::mutate(pred2 = ifelse(Up>THRESHOLD, 'Up', 'Down')) %>%
  dplyr::mutate(pred2 = factor(pred2, levels=c('Down','Up'))) %>%
  dplyr::group_split(Resample) %>%
  purrr::map(~ caret::confusionMatrix(data=.x$pred2, reference=.x$obs))

out_of_folds_CM_vis <- out_of_folds_CM %>% 
  purrr::map(~ .x$table %>% broom::tidy() %>%
                 ggplot(aes(x=Reference, y=Prediction, fill=n, label=n)) +
                 geom_tile() + geom_text(aes(label=n), size=5, color='white'))
 
gridExtra::grid.arrange(grobs=out_of_folds_CM_vis, ncol=2)
```

From the 10-Fold cross validation we can see we have ~130 true positive references to a blue tarp. Slightly, less than the cross validation with KNN. We can measure the sensitivity and the 1-specificity from the initial default threshold is `r toString(THRESHOLD)`.

```{r}
options(yardstick.event_first = FALSE)
ROC_curve <- predict(classifier, type='prob') %>% 
  yardstick::roc_curve(truth=train$BlueTarp_rest, estimate=Up) %>%
  dplyr::mutate(one_minus_specificity = 1-specificity)

# Sensitivity: TP/(TP+FN)
# 1-Specificity: FP/(TN+FP)

ROC_curve_plot <- ROC_curve %>%
  ggplot(aes(x=one_minus_specificity, y=sensitivity)) + 
  geom_line() + geom_point() +
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC curve')

ggplotly(ROC_curve_plot)

predict(classifier, type='prob') %>% 
  yardstick::roc_auc(truth=train$BlueTarp_rest, Up) 
```


```{r}
ROC_curve %>% head()
predict(classifier, type='prob') %>% head()
```

We can also look at each ROC curve from each fold from the LDA model.

```{r}
ROC_curveS <- classifier$pred %>%
  dplyr::group_split(Resample) %>% 
  purrr::map(~ yardstick::roc_curve(.x, truth=obs, Up)) %>%
  purrr::map(~ dplyr::mutate(.x, one_minus_specificity = 1-specificity)) %>%
  purrr::map(~ geom_line(.x, mapping=aes(one_minus_specificity, sensitivity)))

ROC_curveS[[1]] <- ggplot() + ROC_curveS[[1]]
Reduce("+", ROC_curveS) + 
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC Curves')
```

To find the optimal threshold, well look at using the $F_1$ Score.

```{r}
tuned_call <- function(preds, decision_rule_threshold){
  calls <- rep('Down', length(preds))
  calls[preds>decision_rule_threshold]='Up'
  factor(calls, levels=c('Down','Up'))
}


thresholds = seq(0.01,0.99,.01) # Avoid Threshold Tails
F_1_scores <- rep(NA, length(thresholds))
GMeans <- rep(NA, length(thresholds))

preds <- predict(classifier, type='prob')[,2]
y <- train$BlueTarp_rest
for (i in 1:length(thresholds)){
  yhat <- tuned_call(preds, thresholds[i])
  one_minus_specificity <- mean(yhat[y=='Down']=='Up')
  sensitivity <- mean(yhat[y=='Up']=='Up')
  one_minus_specificity
  sensitivity
  results <- tibble::tibble(y=y, yhat=yhat)
  
  caret::confusionMatrix(yhat, y, positive = "Up") 
  cm <- results %>%yardstick::conf_mat(truth=y, yhat)
  
  trueNegative  = cm$table[1,1]
  falseNegative = cm$table[1,2]
  falsePositive = cm$table[2,1]
  truePositve   = cm$table[2,2]
  
  #paste0("True Positive: ", truePositve )
  #paste0("False Negative: ", falseNegative )
  #paste0("True Negative: ", trueNegative )
  #paste0("False Positive: ", falsePositive )
  conditionPositive = truePositve + falseNegative
  conditionNegative = trueNegative + falsePositive 
  predictedConditionPositive = truePositve +  falsePositive 
  predictedConditionNegative = trueNegative + falseNegative
  
  TotalPopulation = truePositve + falsePositive + trueNegative + falseNegative
  
  prevalence = conditionPositive / TotalPopulation
  accuracy = (truePositve + trueNegative) / TotalPopulation
  PPV = truePositve / predictedConditionPositive
  FDR = falsePositive / predictedConditionPositive
  FOR = falseNegative / predictedConditionNegative
  NPV = trueNegative / predictedConditionNegative
  TPR = truePositve / conditionPositive
  FPR = falsePositive / conditionNegative
  FNR = falseNegative / conditionPositive
  SPC = TNR = trueNegative / conditionNegative
  LR.PLUS = TPR / FPR
  LR.MINUS = FNR/TNR
  DOR = LR.PLUS / LR.MINUS
  
  F_1_score = 2 * ((PPV * TPR)/(PPV + TPR))
  F_1_scores[i] <- F_1_score
  GMeans[i] = sqrt(TPR*SPC)
}

opt_thresh = thresholds[tail(which(na.omit(F_1_scores) == min(na.omit(F_1_scores))), n=1)]

if (opt_thresh < 0.1){opt_thresh = 0.1}
if (opt_thresh > 0.5){opt_thresh = 0.5}

#opt_thresh

#points(opt_thresh, F_1_scores[tail(which(na.omit(F_1_scores) == min(na.omit(F_1_scores))), n=1)], col = 'red', cex = 2, pch = 19)
```


```{r}
plot(thresholds, F_1_scores, xlab = 'Thresholds', ylab = 'F1 Score', type = 'l')
```

```{r}
plot(thresholds, GMeans, xlab = 'Thresholds', ylab = 'Geometric Mean', type = 'l')
thresholds[tail(which(na.omit(GMeans) == max(na.omit(GMeans))), n=1)]
```

```{r}
paste0("Confusion Matrix 0.5")
y <- train$BlueTarp_rest
yhat <- tuned_call(preds, 0.5)
one_minus_specificity <- mean(yhat[y=='Down']=='Up')
sensitivity <- mean(yhat[y=='Up']=='Up')
results <- tibble::tibble(y=y, yhat=yhat)

caret::confusionMatrix(yhat, y, positive = "Up") 
cm <- results %>%yardstick::conf_mat(truth=y, yhat)
```

From the initial threshold we can see:

* True Negative = `r toString(trueNegative )`

* False Negative = `r toString(falseNegative)`

* False Positive = `r toString(falsePositive)`

* True Positive = `r toString(truePositve)`

* Condition Positive = `r toString(conditionPositive)`

* condition Negative = `r toString(conditionNegative)`

* Predicted Condition Positive = `r toString(predictedConditionPositive)`

* Predicted Condition Negative = `r toString(predictedConditionNegative)`

* Total Population = `r toString(TotalPopulation)`

* Prevalence = `r toString(prevalence)`

* Accuracy = `r toString(accuracy)`

* Positive predictive value, precision = `r toString(PPV)`

* False Discovery rate = `r toString(FDR)`

* False Omission Rate = `r toString(FOR)`

* Negative Predictive Value = `r toString(NPV)`

* True Positive Rate, Recall, Sensitivity = `r toString(TPR)`

* False Positive Rate = `r toString(FPR)`

* False Negative Rate = `r toString(FNR)`

* Specificity, Selectivity, True Negative rate = `r toString(SPC)`

* Positive likelihood ratio = `r toString(LR.PLUS)`

* Negative likelihood ratio = `r toString(LR.MINUS)`

* Diagnostic Odds Ratio = `r toString(DOR)`

* $F_1$ Score  = `r toString(F_1_score)`

From our plot we selected our optimal threshold as `r toString(opt_thresh)`. 

```{r}
yhat <- tuned_call(preds, opt_thresh)
one_minus_specificity <- mean(yhat[y=='Down']=='Up')
sensitivity <- mean(yhat[y=='Up']=='Up')
one_minus_specificity
sensitivity
results <- tibble::tibble(y=y, yhat=yhat)

caret::confusionMatrix(yhat, y, positive = "Up") 
cm <- results %>%yardstick::conf_mat(truth=y, yhat)

trueNegative  = cm$table[1,1]
falseNegative = cm$table[1,2]
falsePositive = cm$table[2,1]
truePositve   = cm$table[2,2]

conditionPositive = truePositve + falseNegative
conditionNegative = trueNegative + falsePositive 
predictedConditionPositive = truePositve +  falsePositive 
predictedConditionNegative = trueNegative + falseNegative

TotalPopulation = truePositve + falsePositive + trueNegative + falseNegative

prevalence = conditionPositive / TotalPopulation
accuracy = (truePositve + trueNegative) / TotalPopulation
PPV = truePositve / predictedConditionPositive
FDR = falsePositive / predictedConditionPositive
FOR = falseNegative / predictedConditionNegative
NPV = trueNegative / predictedConditionNegative
TPR = truePositve / conditionPositive
FPR = falsePositive / conditionNegative
FNR = falseNegative / conditionPositive
SPC = TNR = trueNegative / conditionNegative
LR.PLUS = TPR / FPR
LR.MINUS = FNR/TNR
DOR = LR.PLUS / LR.MINUS

F_1_score = 2 * ((PPV * TPR)/(PPV + TPR))
```

Now using the optimal threshold we get:

* True Negative = `r toString(trueNegative )`

* False Negative = `r toString(falseNegative)`

* False Positive = `r toString(falsePositive)`

* True Positive = `r toString(truePositve)`

* Condition Positive = `r toString(conditionPositive)`

* condition Negative = `r toString(conditionNegative)`

* Predicted Condition Positive = `r toString(predictedConditionPositive)`

* Predicted Condition Negative = `r toString(predictedConditionNegative)`

* Total Population = `r toString(TotalPopulation)`

* Prevalence = `r toString(prevalence)`

* Accuracy = `r toString(accuracy)`

* Positive predictive value, precision = `r toString(PPV)`

* False Discovery rate = `r toString(FDR)`

* False Omission Rate = `r toString(FOR)`

* Negative Predictive Value = `r toString(NPV)`

* True Positive Rate, Recall, Sensitivity = `r toString(TPR)`

* False Positive Rate = `r toString(FPR)`

* False Negative Rate = `r toString(FNR)`

* Specificity, Selectivity, True Negative rate = `r toString(SPC)`

* Positive likelihood ratio = `r toString(LR.PLUS)`

* Negative likelihood ratio = `r toString(LR.MINUS)`

* Diagnostic Odds Ratio = `r toString(DOR)`

* $F_1$ Score  = `r toString(F_1_score)`

```{r warning=FALSE}
# confusionMatrix

THRESHOLD <- opt_thresh
out_of_folds_CM <- classifier$pred %>% 
  dplyr::mutate(pred2 = ifelse(Up>THRESHOLD, 'Up', 'Down')) %>%
  dplyr::mutate(pred2 = factor(pred2, levels=c('Down','Up'))) %>%
  dplyr::group_split(Resample) %>%
  purrr::map(~ caret::confusionMatrix(data=.x$pred2, reference=.x$obs))

out_of_folds_CM_vis <- out_of_folds_CM %>% 
  purrr::map(~ .x$table %>% broom::tidy() %>%
                 ggplot(aes(x=Reference, y=Prediction, fill=n, label=n)) +
                 geom_tile() + geom_text(aes(label=n), size=5, color='white'))
 
gridExtra::grid.arrange(grobs=out_of_folds_CM_vis, ncol=2)
```

## Summary Stats LDA

```{r}
options(yardstick.event_first = FALSE)
ROC_curve <- predict(classifier, type='prob') %>% 
  yardstick::roc_curve(truth=train$BlueTarp_rest, estimate=Up) %>%
  dplyr::mutate(one_minus_specificity = 1-specificity)

# Sensitivity: TP/(TP+FN)
# 1-Specificity: FP/(TN+FP)

ROC_curve_plot <- ROC_curve %>%
  ggplot(aes(x=one_minus_specificity, y=sensitivity)) + 
  geom_line() + geom_point() +
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC curve')

ggplotly(ROC_curve_plot)

temp = predict(classifier, type='prob') %>% yardstick::roc_auc(truth=train$BlueTarp_rest, Up) 

lda.Accuracy = accuracy
lda.AUC = temp$.estimate
twoclassSum = twoClassSummary( data = data.frame(obs = classifier$pred$obs,
  pred = classifier$pred$pred,
  Down = classifier$pred$Down, 
  Up = classifier$pred$Up), lev = levels(classifier$pred$obs))

lda.ROC = "TRUE" #unname(twoclassSum[1])
lda.Threshold = opt_thresh
lda.Sensitivity = lda.Recall= lda.Power =  TPR
lda.Specificity= 1-FPR
lda.FDR = FDR
lda.Precision=PPV
```

```{r}
df = data.frame(c(lda.Accuracy, lda.AUC, lda.ROC, lda.Threshold, lda.Sensitivity, lda.Specificity, lda.FDR , lda.Precision))

colnames(df) <- c("LDA")
rownames(df) <- c("Accuracy","AUC", "ROC", "Threshold","Sensitivity=Recall=Power","Specificity=1-FPR","FDR","Precision=PPV")
df %>% kbl() %>% kable_styling()
```

# QDA

The next classification process that we used is the quadratic discriminant analysis (qda). Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. Again, we’ll use the caret package in R to apply the qda classification procedure using a 10 - Fold Cross-Validation evaluation procedure. 

```{r caret QDA}
set.seed(1)
train$BlueTarp_rest <- as.factor(train$BlueTarp_rest)
levels(train$BlueTarp_rest) <- c("Down","Up")
# https://blog.revolutionanalytics.com/2016/05/using-caret-to-compare-models.html

classifier <- caret::train(BlueTarp_rest ~ Red + Green + Blue, data=train, method="qda", 
                           preProcess=c("center","scale"),
                           #tuneGrid=data.frame(.dimen = seq(1,10,1)),#seq(1,1000,5)),
                           trControl = caret::trainControl("cv", number=kfoldcv, 
                                                           returnResamp='all', savePredictions='final',
                                                           classProbs=TRUE))

```

```{r}
classifier
```

```{r message=FALSE, warning=FALSE}
classifier$resample %>% 
  dplyr::group_split(Resample) %>% 
  purrr::map(rowid_to_column) %>%
  dplyr::bind_rows() %>%
  ggplot(aes(rowid, Accuracy)) + geom_point() +
  geom_smooth(formula='y~x', method='loess', span=.03) +
  geom_line(classifier$results, mapping=aes(seq_along(Accuracy), Accuracy),
            size=2, color='red')
```

Next we'll look at the Confusion Matrix from the model. 

```{r warning=FALSE}
# confusionMatrix
THRESHOLD <- 0.5
out_of_folds_CM <- classifier$pred %>% 
  dplyr::mutate(pred2 = ifelse(Up>THRESHOLD, 'Up', 'Down')) %>%
  dplyr::mutate(pred2 = factor(pred2, levels=c('Down','Up'))) %>%
  dplyr::group_split(Resample) %>%
  purrr::map(~ caret::confusionMatrix(data=.x$pred2, reference=.x$obs))

out_of_folds_CM_vis <- out_of_folds_CM %>% 
  purrr::map(~ .x$table %>% broom::tidy() %>%
                 ggplot(aes(x=Reference, y=Prediction, fill=n, label=n)) +
                 geom_tile() + geom_text(aes(label=n), size=5, color='white'))
 
gridExtra::grid.arrange(grobs=out_of_folds_CM_vis, ncol=2)
```

From the 10-Fold cross validation we can see we have  true positive references to a blue tarp on par with what we see in LDA. We can measure the sensitivity and the 1-specificity from the initial default threshold is `r toString(THRESHOLD)`.

```{r}
options(yardstick.event_first = FALSE)
ROC_curve <- predict(classifier, type='prob') %>% 
  yardstick::roc_curve(truth=train$BlueTarp_rest, estimate=Up) %>%
  dplyr::mutate(one_minus_specificity = 1-specificity)

# Sensitivity: TP/(TP+FN)
# 1-Specificity: FP/(TN+FP)

ROC_curve_plot <- ROC_curve %>%
  ggplot(aes(x=one_minus_specificity, y=sensitivity)) + 
  geom_line() + geom_point() +
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC curve')

ggplotly(ROC_curve_plot)

predict(classifier, type='prob') %>% 
  yardstick::roc_auc(truth=train$BlueTarp_rest, Up) 
```

```{r}
ROC_curve %>% head()
predict(classifier, type='prob') %>% head()
```

We can also look at each ROC curve from each fold from the QDA model.

```{r}
ROC_curveS <- classifier$pred %>%
  dplyr::group_split(Resample) %>% 
  purrr::map(~ yardstick::roc_curve(.x, truth=obs, Up)) %>%
  purrr::map(~ dplyr::mutate(.x, one_minus_specificity = 1-specificity)) %>%
  purrr::map(~ geom_line(.x, mapping=aes(one_minus_specificity, sensitivity)))

ROC_curveS[[1]] <- ggplot() + ROC_curveS[[1]]
Reduce("+", ROC_curveS) + 
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC Curves')
```

To find the optimal threshold, well look at using the $F_1$ Score.

```{r}
tuned_call <- function(preds, decision_rule_threshold){
  calls <- rep('Down', length(preds))
  calls[preds>decision_rule_threshold]='Up'
  factor(calls, levels=c('Down','Up'))
}


thresholds = seq(0.01,0.99,.01) # Avoid Threshold Tails
F_1_scores <- rep(NA, length(thresholds))
GMeans <- rep(NA, length(thresholds))

preds <- predict(classifier, type='prob')[,2]
y <- train$BlueTarp_rest
for (i in 1:length(thresholds)){
  yhat <- tuned_call(preds, thresholds[i])
  one_minus_specificity <- mean(yhat[y=='Down']=='Up')
  sensitivity <- mean(yhat[y=='Up']=='Up')
  one_minus_specificity
  sensitivity
  results <- tibble::tibble(y=y, yhat=yhat)
  
  caret::confusionMatrix(yhat, y, positive = "Up") 
  cm <- results %>%yardstick::conf_mat(truth=y, yhat)
  
  trueNegative  = cm$table[1,1]
  falseNegative = cm$table[1,2]
  falsePositive = cm$table[2,1]
  truePositve   = cm$table[2,2]
  
  #paste0("True Positive: ", truePositve )
  #paste0("False Negative: ", falseNegative )
  #paste0("True Negative: ", trueNegative )
  #paste0("False Positive: ", falsePositive )
  conditionPositive = truePositve + falseNegative
  conditionNegative = trueNegative + falsePositive 
  predictedConditionPositive = truePositve +  falsePositive 
  predictedConditionNegative = trueNegative + falseNegative
  
  TotalPopulation = truePositve + falsePositive + trueNegative + falseNegative
  
  prevalence = conditionPositive / TotalPopulation
  accuracy = (truePositve + trueNegative) / TotalPopulation
  PPV = truePositve / predictedConditionPositive
  FDR = falsePositive / predictedConditionPositive
  FOR = falseNegative / predictedConditionNegative
  NPV = trueNegative / predictedConditionNegative
  TPR = truePositve / conditionPositive
  FPR = falsePositive / conditionNegative
  FNR = falseNegative / conditionPositive
  SPC = TNR = trueNegative / conditionNegative
  LR.PLUS = TPR / FPR
  LR.MINUS = FNR/TNR
  DOR = LR.PLUS / LR.MINUS
  
  F_1_score = 2 * ((PPV * TPR)/(PPV + TPR))
  F_1_scores[i] <- F_1_score
  GMeans[i] = sqrt(TPR*SPC)
}

opt_thresh = thresholds[tail(which(na.omit(F_1_scores) == min(na.omit(F_1_scores))), n=1)]

if (opt_thresh < 0.1){opt_thresh = 0.1}
if (opt_thresh > 0.5){opt_thresh = 0.5}

#opt_thresh

#points(opt_thresh, F_1_scores[tail(which(na.omit(F_1_scores) == min(na.omit(F_1_scores))), n=1)], col = 'red', cex = 2, pch = 19)
```


```{r}
plot(thresholds, F_1_scores, xlab = 'Thresholds', ylab = 'F1 Score', type = 'l')
```

```{r}
plot(thresholds, GMeans, xlab = 'Thresholds', ylab = 'Geometric Mean', type = 'l')
thresholds[tail(which(na.omit(GMeans) == max(na.omit(GMeans))), n=1)]
```

```{r}
paste0("Confusion Matrix 0.5")
y <- train$BlueTarp_rest
yhat <- tuned_call(preds, 0.5)
one_minus_specificity <- mean(yhat[y=='Down']=='Up')
sensitivity <- mean(yhat[y=='Up']=='Up')
results <- tibble::tibble(y=y, yhat=yhat)

caret::confusionMatrix(yhat, y, positive = "Up") 
cm <- results %>%yardstick::conf_mat(truth=y, yhat)
```

From the initial threshold we can see:

* True Negative = `r toString(trueNegative )`

* False Negative = `r toString(falseNegative)`

* False Positive = `r toString(falsePositive)`

* True Positive = `r toString(truePositve)`

* Condition Positive = `r toString(conditionPositive)`

* condition Negative = `r toString(conditionNegative)`

* Predicted Condition Positive = `r toString(predictedConditionPositive)`

* Predicted Condition Negative = `r toString(predictedConditionNegative)`

* Total Population = `r toString(TotalPopulation)`

* Prevalence = `r toString(prevalence)`

* Accuracy = `r toString(accuracy)`

* Positive predictive value, precision = `r toString(PPV)`

* False Discovery rate = `r toString(FDR)`

* False Omission Rate = `r toString(FOR)`

* Negative Predictive Value = `r toString(NPV)`

* True Positive Rate, Recall, Sensitivity = `r toString(TPR)`

* False Positive Rate = `r toString(FPR)`

* False Negative Rate = `r toString(FNR)`

* Specificity, Selectivity, True Negative rate = `r toString(SPC)`

* Positive likelihood ratio = `r toString(LR.PLUS)`

* Negative likelihood ratio = `r toString(LR.MINUS)`

* Diagnostic Odds Ratio = `r toString(DOR)`

* $F_1$ Score  = `r toString(F_1_score)`

From our plot we selected our optimal threshold as `r toString(opt_thresh)`. 

```{r}
yhat <- tuned_call(preds, opt_thresh)
one_minus_specificity <- mean(yhat[y=='Down']=='Up')
sensitivity <- mean(yhat[y=='Up']=='Up')
one_minus_specificity
sensitivity
results <- tibble::tibble(y=y, yhat=yhat)

caret::confusionMatrix(yhat, y, positive = "Up") 
cm <- results %>%yardstick::conf_mat(truth=y, yhat)

trueNegative  = cm$table[1,1]
falseNegative = cm$table[1,2]
falsePositive = cm$table[2,1]
truePositve   = cm$table[2,2]

conditionPositive = truePositve + falseNegative
conditionNegative = trueNegative + falsePositive 
predictedConditionPositive = truePositve +  falsePositive 
predictedConditionNegative = trueNegative + falseNegative

TotalPopulation = truePositve + falsePositive + trueNegative + falseNegative

prevalence = conditionPositive / TotalPopulation
accuracy = (truePositve + trueNegative) / TotalPopulation
PPV = truePositve / predictedConditionPositive
FDR = falsePositive / predictedConditionPositive
FOR = falseNegative / predictedConditionNegative
NPV = trueNegative / predictedConditionNegative
TPR = truePositve / conditionPositive
FPR = falsePositive / conditionNegative
FNR = falseNegative / conditionPositive
SPC = TNR = trueNegative / conditionNegative
LR.PLUS = TPR / FPR
LR.MINUS = FNR/TNR
DOR = LR.PLUS / LR.MINUS

F_1_score = 2 * ((PPV * TPR)/(PPV + TPR))
```

Now using the optimal threshold we get:

* True Negative = `r toString(trueNegative )`

* False Negative = `r toString(falseNegative)`

* False Positive = `r toString(falsePositive)`

* True Positive = `r toString(truePositve)`

* Condition Positive = `r toString(conditionPositive)`

* condition Negative = `r toString(conditionNegative)`

* Predicted Condition Positive = `r toString(predictedConditionPositive)`

* Predicted Condition Negative = `r toString(predictedConditionNegative)`

* Total Population = `r toString(TotalPopulation)`

* Prevalence = `r toString(prevalence)`

* Accuracy = `r toString(accuracy)`

* Positive predictive value, precision = `r toString(PPV)`

* False Discovery rate = `r toString(FDR)`

* False Omission Rate = `r toString(FOR)`

* Negative Predictive Value = `r toString(NPV)`

* True Positive Rate, Recall, Sensitivity = `r toString(TPR)`

* False Positive Rate = `r toString(FPR)`

* False Negative Rate = `r toString(FNR)`

* Specificity, Selectivity, True Negative rate = `r toString(SPC)`

* Positive likelihood ratio = `r toString(LR.PLUS)`

* Negative likelihood ratio = `r toString(LR.MINUS)`

* Diagnostic Odds Ratio = `r toString(DOR)`

* $F_1$ Score  = `r toString(F_1_score)`

```{r warning=FALSE}
# confusionMatrix

THRESHOLD <- opt_thresh
out_of_folds_CM <- classifier$pred %>% 
  dplyr::mutate(pred2 = ifelse(Up>THRESHOLD, 'Up', 'Down')) %>%
  dplyr::mutate(pred2 = factor(pred2, levels=c('Down','Up'))) %>%
  dplyr::group_split(Resample) %>%
  purrr::map(~ caret::confusionMatrix(data=.x$pred2, reference=.x$obs))

out_of_folds_CM_vis <- out_of_folds_CM %>% 
  purrr::map(~ .x$table %>% broom::tidy() %>%
                 ggplot(aes(x=Reference, y=Prediction, fill=n, label=n)) +
                 geom_tile() + geom_text(aes(label=n), size=5, color='white'))
 
gridExtra::grid.arrange(grobs=out_of_folds_CM_vis, ncol=2)
```

## Summary Stats QDA

```{r}
options(yardstick.event_first = FALSE)
ROC_curve <- predict(classifier, type='prob') %>% 
  yardstick::roc_curve(truth=train$BlueTarp_rest, estimate=Up) %>%
  dplyr::mutate(one_minus_specificity = 1-specificity)

# Sensitivity: TP/(TP+FN)
# 1-Specificity: FP/(TN+FP)

ROC_curve_plot <- ROC_curve %>%
  ggplot(aes(x=one_minus_specificity, y=sensitivity)) + 
  geom_line() + geom_point() +
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC curve')

ggplotly(ROC_curve_plot)

temp = predict(classifier, type='prob') %>% yardstick::roc_auc(truth=train$BlueTarp_rest, Up) 

qda.Accuracy = accuracy
qda.AUC = temp$.estimate
twoclassSum = twoClassSummary( data = data.frame(obs = classifier$pred$obs,
  pred = classifier$pred$pred,
  Down = classifier$pred$Down, 
  Up = classifier$pred$Up), lev = levels(classifier$pred$obs))

qda.ROC = "TRUE" #unname(twoclassSum[1])
qda.Threshold = opt_thresh
qda.Sensitivity = qda.Recall= qda.Power =  TPR
qda.Specificity= 1-FPR
qda.FDR = FDR
qda.Precision=PPV
```

```{r}
df = data.frame(c(qda.Accuracy, qda.AUC, qda.ROC, qda.Threshold, qda.Sensitivity, qda.Specificity, qda.FDR , qda.Precision))

colnames(df) <- c("QDA")
rownames(df) <- c("Accuracy","AUC", "ROC", "Threshold","Sensitivity=Recall=Power","Specificity=1-FPR","FDR","Precision=PPV")
df %>% kbl() %>% kable_styling()
```

# Logistic Regression

Our final classification process that we used is the linear discriminant analysis (glm). Again, we’ll use the caret package in R to apply the glm classification procedure using a 10 - Fold Cross-Validation evaluation procedure. 

```{r caret GLM, warning=FALSE}
set.seed(1)
train$BlueTarp_rest <- as.factor(train$BlueTarp_rest)
levels(train$BlueTarp_rest) <- c("Down","Up")
# https://blog.revolutionanalytics.com/2016/05/using-caret-to-compare-models.html

classifier <- caret::train(BlueTarp_rest ~ Red + Green + Blue, data=train, method="glm", family=binomial(),
      preProcess=c("center","scale"), trControl = caret::trainControl("cv", number=kfoldcv, 
      returnResamp='all', savePredictions='final',classProbs=TRUE))
```

```{r}
classifier
```

```{r message=FALSE, warning=FALSE}
classifier$resample %>% 
  dplyr::group_split(Resample) %>% 
  purrr::map(rowid_to_column) %>%
  dplyr::bind_rows() %>%
  ggplot(aes(rowid, Accuracy)) + geom_point() +
  geom_smooth(formula='y~x', method='loess', span=.03) +
  geom_line(classifier$results, mapping=aes(seq_along(Accuracy), Accuracy),
            size=2, color='red')
```

Next we'll look at the Confusion Matrix from the model. 

```{r warning=FALSE}
# confusionMatrix
THRESHOLD <- 0.5
out_of_folds_CM <- classifier$pred %>% 
  dplyr::mutate(pred2 = ifelse(Up>THRESHOLD, 'Up', 'Down')) %>%
  dplyr::mutate(pred2 = factor(pred2, levels=c('Down','Up'))) %>%
  dplyr::group_split(Resample) %>%
  purrr::map(~ caret::confusionMatrix(data=.x$pred2, reference=.x$obs))

out_of_folds_CM_vis <- out_of_folds_CM %>% 
  purrr::map(~ .x$table %>% broom::tidy() %>%
                 ggplot(aes(x=Reference, y=Prediction, fill=n, label=n)) +
                 geom_tile() + geom_text(aes(label=n), size=5, color='white'))
 
gridExtra::grid.arrange(grobs=out_of_folds_CM_vis, ncol=2)
```

From the 10-Fold cross validation we can see we have ~130-140 true positive references to a blue tarp. We can measure the sensitivity and the 1-specificity from the initial default threshold is `r toString(THRESHOLD)`.

```{r}
options(yardstick.event_first = FALSE)
ROC_curve <- predict(classifier, type='prob') %>% 
  yardstick::roc_curve(truth=train$BlueTarp_rest, estimate=Up) %>%
  dplyr::mutate(one_minus_specificity = 1-specificity)

# Sensitivity: TP/(TP+FN)
# 1-Specificity: FP/(TN+FP)

ROC_curve_plot <- ROC_curve %>%
  ggplot(aes(x=one_minus_specificity, y=sensitivity)) + 
  geom_line() + geom_point() +
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC curve')

ggplotly(ROC_curve_plot)

predict(classifier, type='prob') %>% 
  yardstick::roc_auc(truth=train$BlueTarp_rest, Up) 
```


```{r}
ROC_curve %>% head()
predict(classifier, type='prob') %>% head()
```

We can also look at each ROC curve from each fold from the glm model.

```{r}
ROC_curveS <- classifier$pred %>%
  dplyr::group_split(Resample) %>% 
  purrr::map(~ yardstick::roc_curve(.x, truth=obs, Up)) %>%
  purrr::map(~ dplyr::mutate(.x, one_minus_specificity = 1-specificity)) %>%
  purrr::map(~ geom_line(.x, mapping=aes(one_minus_specificity, sensitivity)))

ROC_curveS[[1]] <- ggplot() + ROC_curveS[[1]]
Reduce("+", ROC_curveS) + 
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC Curves')
```

To find the optimal threshold, well look at using the $F_1$ Score.

```{r}
tuned_call <- function(preds, decision_rule_threshold){
  calls <- rep('Down', length(preds))
  calls[preds>decision_rule_threshold]='Up'
  factor(calls, levels=c('Down','Up'))
}


thresholds = seq(0.01,0.99,.01) # Avoid Threshold Tails
F_1_scores <- rep(NA, length(thresholds))
GMeans <- rep(NA, length(thresholds))

preds <- predict(classifier, type='prob')[,2]
y <- train$BlueTarp_rest
for (i in 1:length(thresholds)){
  yhat <- tuned_call(preds, thresholds[i])
  one_minus_specificity <- mean(yhat[y=='Down']=='Up')
  sensitivity <- mean(yhat[y=='Up']=='Up')
  one_minus_specificity
  sensitivity
  results <- tibble::tibble(y=y, yhat=yhat)
  
  caret::confusionMatrix(yhat, y, positive = "Up") 
  cm <- results %>%yardstick::conf_mat(truth=y, yhat)
  
  trueNegative  = cm$table[1,1]
  falseNegative = cm$table[1,2]
  falsePositive = cm$table[2,1]
  truePositve   = cm$table[2,2]
  
  #paste0("True Positive: ", truePositve )
  #paste0("False Negative: ", falseNegative )
  #paste0("True Negative: ", trueNegative )
  #paste0("False Positive: ", falsePositive )
  conditionPositive = truePositve + falseNegative
  conditionNegative = trueNegative + falsePositive 
  predictedConditionPositive = truePositve +  falsePositive 
  predictedConditionNegative = trueNegative + falseNegative
  
  TotalPopulation = truePositve + falsePositive + trueNegative + falseNegative
  
  prevalence = conditionPositive / TotalPopulation
  accuracy = (truePositve + trueNegative) / TotalPopulation
  PPV = truePositve / predictedConditionPositive
  FDR = falsePositive / predictedConditionPositive
  FOR = falseNegative / predictedConditionNegative
  NPV = trueNegative / predictedConditionNegative
  TPR = truePositve / conditionPositive
  FPR = falsePositive / conditionNegative
  FNR = falseNegative / conditionPositive
  SPC = TNR = trueNegative / conditionNegative
  LR.PLUS = TPR / FPR
  LR.MINUS = FNR/TNR
  DOR = LR.PLUS / LR.MINUS
  
  F_1_score = 2 * ((PPV * TPR)/(PPV + TPR))
  F_1_scores[i] <- F_1_score
  GMeans[i] = sqrt(TPR*SPC)
}

opt_thresh = thresholds[tail(which(na.omit(F_1_scores) == min(na.omit(F_1_scores))), n=1)]

if (opt_thresh < 0.1){opt_thresh = 0.1}
if (opt_thresh > 0.5){opt_thresh = 0.5}

#opt_thresh

#points(opt_thresh, F_1_scores[tail(which(na.omit(F_1_scores) == min(na.omit(F_1_scores))), n=1)], col = 'red', cex = 2, pch = 19)
```


```{r}
plot(thresholds, F_1_scores, xlab = 'Thresholds', ylab = 'F1 Score', type = 'l')
```

```{r}
plot(thresholds, GMeans, xlab = 'Thresholds', ylab = 'Geometric Mean', type = 'l')
thresholds[tail(which(na.omit(GMeans) == max(na.omit(GMeans))), n=1)]
```

```{r}
paste0("Confusion Matrix 0.5")
y <- train$BlueTarp_rest
yhat <- tuned_call(preds, 0.5)
one_minus_specificity <- mean(yhat[y=='Down']=='Up')
sensitivity <- mean(yhat[y=='Up']=='Up')
results <- tibble::tibble(y=y, yhat=yhat)

caret::confusionMatrix(yhat, y, positive = "Up") 
cm <- results %>%yardstick::conf_mat(truth=y, yhat)
```

From the initial threshold we can see:

* True Negative = `r toString(trueNegative )`

* False Negative = `r toString(falseNegative)`

* False Positive = `r toString(falsePositive)`

* True Positive = `r toString(truePositve)`

* Condition Positive = `r toString(conditionPositive)`

* condition Negative = `r toString(conditionNegative)`

* Predicted Condition Positive = `r toString(predictedConditionPositive)`

* Predicted Condition Negative = `r toString(predictedConditionNegative)`

* Total Population = `r toString(TotalPopulation)`

* Prevalence = `r toString(prevalence)`

* Accuracy = `r toString(accuracy)`

* Positive predictive value, precision = `r toString(PPV)`

* False Discovery rate = `r toString(FDR)`

* False Omission Rate = `r toString(FOR)`

* Negative Predictive Value = `r toString(NPV)`

* True Positive Rate, Recall, Sensitivity = `r toString(TPR)`

* False Positive Rate = `r toString(FPR)`

* False Negative Rate = `r toString(FNR)`

* Specificity, Selectivity, True Negative rate = `r toString(SPC)`

* Positive likelihood ratio = `r toString(LR.PLUS)`

* Negative likelihood ratio = `r toString(LR.MINUS)`

* Diagnostic Odds Ratio = `r toString(DOR)`

* $F_1$ Score  = `r toString(F_1_score)`

From our plot we selected our optimal threshold as `r toString(opt_thresh)`. 

```{r}
yhat <- tuned_call(preds, opt_thresh)
one_minus_specificity <- mean(yhat[y=='Down']=='Up')
sensitivity <- mean(yhat[y=='Up']=='Up')
one_minus_specificity
sensitivity
results <- tibble::tibble(y=y, yhat=yhat)

caret::confusionMatrix(yhat, y, positive = "Up") 
cm <- results %>%yardstick::conf_mat(truth=y, yhat)

trueNegative  = cm$table[1,1]
falseNegative = cm$table[1,2]
falsePositive = cm$table[2,1]
truePositve   = cm$table[2,2]

conditionPositive = truePositve + falseNegative
conditionNegative = trueNegative + falsePositive 
predictedConditionPositive = truePositve +  falsePositive 
predictedConditionNegative = trueNegative + falseNegative

TotalPopulation = truePositve + falsePositive + trueNegative + falseNegative

prevalence = conditionPositive / TotalPopulation
accuracy = (truePositve + trueNegative) / TotalPopulation
PPV = truePositve / predictedConditionPositive
FDR = falsePositive / predictedConditionPositive
FOR = falseNegative / predictedConditionNegative
NPV = trueNegative / predictedConditionNegative
TPR = truePositve / conditionPositive
FPR = falsePositive / conditionNegative
FNR = falseNegative / conditionPositive
SPC = TNR = trueNegative / conditionNegative
LR.PLUS = TPR / FPR
LR.MINUS = FNR/TNR
DOR = LR.PLUS / LR.MINUS

F_1_score = 2 * ((PPV * TPR)/(PPV + TPR))
```

Now using the optimal threshold we get:

* True Negative = `r toString(trueNegative )`

* False Negative = `r toString(falseNegative)`

* False Positive = `r toString(falsePositive)`

* True Positive = `r toString(truePositve)`

* Condition Positive = `r toString(conditionPositive)`

* condition Negative = `r toString(conditionNegative)`

* Predicted Condition Positive = `r toString(predictedConditionPositive)`

* Predicted Condition Negative = `r toString(predictedConditionNegative)`

* Total Population = `r toString(TotalPopulation)`

* Prevalence = `r toString(prevalence)`

* Accuracy = `r toString(accuracy)`

* Positive predictive value, precision = `r toString(PPV)`

* False Discovery rate = `r toString(FDR)`

* False Omission Rate = `r toString(FOR)`

* Negative Predictive Value = `r toString(NPV)`

* True Positive Rate, Recall, Sensitivity = `r toString(TPR)`

* False Positive Rate = `r toString(FPR)`

* False Negative Rate = `r toString(FNR)`

* Specificity, Selectivity, True Negative rate = `r toString(SPC)`

* Positive likelihood ratio = `r toString(LR.PLUS)`

* Negative likelihood ratio = `r toString(LR.MINUS)`

* Diagnostic Odds Ratio = `r toString(DOR)`

* $F_1$ Score  = `r toString(F_1_score)`

```{r warning=FALSE}
# confusionMatrix

THRESHOLD <- opt_thresh
out_of_folds_CM <- classifier$pred %>% 
  dplyr::mutate(pred2 = ifelse(Up>THRESHOLD, 'Up', 'Down')) %>%
  dplyr::mutate(pred2 = factor(pred2, levels=c('Down','Up'))) %>%
  dplyr::group_split(Resample) %>%
  purrr::map(~ caret::confusionMatrix(data=.x$pred2, reference=.x$obs))

out_of_folds_CM_vis <- out_of_folds_CM %>% 
  purrr::map(~ .x$table %>% broom::tidy() %>%
                 ggplot(aes(x=Reference, y=Prediction, fill=n, label=n)) +
                 geom_tile() + geom_text(aes(label=n), size=5, color='white'))
 
gridExtra::grid.arrange(grobs=out_of_folds_CM_vis, ncol=2)
```

## Summary Stats Logistic Regression

```{r}
options(yardstick.event_first = FALSE)
ROC_curve <- predict(classifier, type='prob') %>% 
  yardstick::roc_curve(truth=train$BlueTarp_rest, estimate=Up) %>%
  dplyr::mutate(one_minus_specificity = 1-specificity)

# Sensitivity: TP/(TP+FN)
# 1-Specificity: FP/(TN+FP)

ROC_curve_plot <- ROC_curve %>%
  ggplot(aes(x=one_minus_specificity, y=sensitivity)) + 
  geom_line() + geom_point() +
  geom_abline(slope=1, intercept=0, linetype='dashed', color='red') +
  xlab("one_minus_specificity\n(false positive rate)") +
  ggtitle('ROC curve')

ggplotly(ROC_curve_plot)

temp = predict(classifier, type='prob') %>% yardstick::roc_auc(truth=train$BlueTarp_rest, Up) 

glm.Accuracy = accuracy
glm.AUC = temp$.estimate
twoclassSum = twoClassSummary( data = data.frame(obs = classifier$pred$obs,
  pred = classifier$pred$pred,
  Down = classifier$pred$Down, 
  Up = classifier$pred$Up), lev = levels(classifier$pred$obs))

glm.ROC = "TRUE" #unname(twoclassSum[1])
glm.Threshold = opt_thresh
glm.Sensitivity = glm.Recall= glm.Power =  TPR
glm.Specificity= 1-FPR
glm.FDR = FDR
glm.Precision=PPV
```

```{r}
df = data.frame(c(glm.Accuracy, glm.AUC, glm.ROC, glm.Threshold, glm.Sensitivity, glm.Specificity, glm.FDR , glm.Precision))

colnames(df) <- c("Logistic Regression")
rownames(df) <- c("Accuracy","AUC", "ROC", "Threshold","Sensitivity=Recall=Power","Specificity=1-FPR","FDR","Precision=PPV")
df %>% kbl() %>% kable_styling()
```

# Conclusions

```{r}

df = data.frame(c(knn.Accuracy, knn.AUC, knn.ROC, knn.Threshold, knn.Sensitivity, knn.Specificity, knn.FDR , knn.Precision),
                c(lda.Accuracy, lda.AUC, lda.ROC, lda.Threshold, lda.Sensitivity, lda.Specificity, lda.FDR , lda.Precision),
                c(qda.Accuracy, qda.AUC, qda.ROC, qda.Threshold, qda.Sensitivity, qda.Specificity, qda.FDR , qda.Precision), 
                c(glm.Accuracy, glm.AUC, glm.ROC, glm.Threshold, glm.Sensitivity, glm.Specificity, glm.FDR , glm.Precision))


colnames(df) <- c(paste0("KNN (k =",bestk,")"), "LDA","QDA","Logistic Regression")
rownames(df) <- c("Accuracy","AUC", "ROC", "Threshold","Sensitivity=Recall=Power","Specificity=1-FPR","FDR","Precision=PPV")
df %>% kbl() %>% kable_styling()
```

In order to determine the the overall optimal classification method, we'll use the scoring system of 5 for first place, 3 for second, 2 for third, and 1 for fourth to rank each model by select various statistics.

If we look at the statistics provided, accuracy is a measure of $\frac{True Positive + TrueNegative}{TotalPopulation}$. So the optimal model should be one that has highest accuracy

KNN (k = `r toString(bestk)` ) | LDA | QDA | Logistic Regression
------------- | -------------|------------- | -------------
5|1|2|3
------------- | -------------|------------- | -------------
5|1|2|3

AUC, or Area Under Curve, is a measure of performance across all possible classification thresholds. Lowering the classification threshold classifies more items as positive. As such, you'll want a model with a high AUC value. 

KNN (k = `r toString(bestk)` ) | LDA | QDA | Logistic Regression
------------- | -------------|------------- | -------------
5|1|2|3
------------- | -------------|------------- | -------------
10|2|4|6

Using the same threshold of `r toString(opt_thresh)` across the same training set, we can compare the sensitivities and Specificities.

Sensitivity is defined as $\frac{True Positive}{TruePositive + FalseNegative}$. 
Specificity is defined as $\frac{True Negative}{FalsePositive + TrueNegative}$. 

Both of which, you'd want to have high values in order to correctly predict observations.

KNN (k = `r toString(bestk)` ) | LDA | QDA | Logistic Regression
------------- | -------------|------------- | -------------
5|1|2|3
------------- | -------------|------------- | -------------
15|3|6|9


KNN (k = `r toString(bestk)` ) | LDA | QDA | Logistic Regression
------------- | -------------|------------- | -------------
5|1|3|2
------------- | -------------|------------- | -------------
20|4|9|11

The False Discovery Rate (FDR) is the expected proportion of type I errors ( you get a false positive ). The optimal model is one with a minimum value for FDR. 

KNN (k = `r toString(bestk)` ) | LDA | QDA | Logistic Regression
------------- | -------------|------------- | -------------
5|1|3|2
------------- | -------------|------------- | -------------
25|5|12|13

Precision is a measure of $\frac{True Positive}{FalsePositive + TruePositive}$. Ideally, you'll want a high value of precision.

KNN (k = `r toString(bestk)` ) | LDA | QDA | Logistic Regression
------------- | -------------|------------- | -------------
5|1|3|2
------------- | -------------|------------- | -------------
30|6|15|15

The determination of which algorithm works best KNN was shown to be the optimal classifier, followed by logistic regression / quadratic discriminant analysis and lastly linear discriminant analysis. Although KNN performed the best, the logistic regression / quadratic discriminant analysis models proved to have a comparative accuracy and precision. These models have higher FDR, but this is due to the fact of the selected threshold. With QDA and Logistic Regression performing better than LDA, we can make the assumption that the data has a non-linear boundary between classes.  For much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. In addition, the KNN model also supports this assumption since KNN typically performs better for non-linear data classification. When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well. When the boundaries are moderately non-linear, QDA may give better results. 

An additional recommended action that might be taken to improve results is to select a threshold that may be slightly more precise and compared for each classifier. As such, I opted to use the $F_1$ Score since it can be used to measure the performance of the unbalanced classification data set and cross referenced that with the use of the geometric mean since it can "indicate a poor performance in the classification of the positive cases even if the negative cases are correctly classified as such" (Akosa). The more detailed thresholds appeared to be less than 0.1, however you lose some flexibility in applying future data to the generated classifiers.

The variables associated with this data set are additive colors of the RBG color spectrum. This data formulation allows us to address it with predictive modeling tools since the variables stem from the data being an unbalanced  binary classification ( an unequal distribution of classes in the training dataset). Since the goal was to find an algorithm that could effectively search the images in order to locate displaced persons and communicate those locations rescue workers so they could help those who needed it in time, the work show above can be used to find these persons and administer aide. Due to limited resources, it would not be feasible to save everyone, but adjusting the threshold would allow us to save more persons than leaving it as default. 

In regards to software aspects, we expect knn to take the longest performance. The time required to do 10-fold cross validation adjusted for using tuning parameters far exceed those for LDA, GDA and GLM. However it is worth noting, that when running KNN for 1 singular value of K, the performance time been the different classification models is negligible. Furthermore, you may notice that in my confusion matrices I used: caret, yardstick and mean. All 3 produced nearly the same number of True Positives at a given threshold yet a few instances will vary. This may be due to how the underlining architecture handles floating point precision and random number generations ( especially if any step re-randomizes the training data). My calculations tend to have a large number of significant digits but if rounding to a lesser number of significant digits, the values are comparable. Also, since the goal of the analysis was to increase the number of True Positives with changes to threshold, was deemed a fair trade-off if we raised the number of these in each model.

# References

* https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/

* https://machinelearningmastery.com/what-is-imbalanced-classification/

* https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc

* https://www.statisticshowto.com/false-discovery-rate/

* https://stats.stackexchange.com/questions/49226/how-to-interpret-f-measure-values

* https://support.sas.com/resources/papers/proceedings17/0942-2017.pdf
